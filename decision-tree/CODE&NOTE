▩ 5장. 의사결정트리

의사결정트리 ? 데이터들이 가진 속성들로부터 분할 기준 속성을 판별하고,
			      분할 기준 속성에 따라 트리 형태로 모델링하는 분류 예측 모델을 의사결정트리 모델이라고 한다.

회귀분석, 의사결정 트리는 현업에서 고객들과 데이터 분석가들이 선호하는 머신러닝 알고리즘 입니다.
신경망이 정확도를 훨씬 뛰어난데 신경망의 경우는 신경망 내부가 블랙박스이다 보니
설명이 안되서 고객들이 이해를 못하는 경우가 많습니다. 
의사결정트리와 회귀분석은 설명이 가능해서 왜 이렇게 예측하고 분류했는지 설명이 가능해서 선호하는
분류모델 입니다.


					표                                                                               의사결정트리 그림
		수중              지느러미             물고기       
		1. YES                 YES                        YES                                                            수중
		2. YES                 NO                         NO                                                         /              \
		3. NO                  YES                        NO                                                      NO               지느러미
		4. NO                  NO                         NO                                                                     /         \
																		NO                        YES
															    : 예측 결과 ,     : 분할 기준 속성


의사결정트리는 어떻게 데이터를 분류하는가 ?

		1. 정보획득량이 높은 컬럼을 선별하고 그것부터 먼저 물어봅니다.
		2. 물어보면서 자식 노드로 내려올때 데이터의 분류가 점점 명확해져야 되므로
		부모 노드 때 보다 자식 노드때의 데이터가 더 불순도( entropy ) 가 낮아져야 합니다.
		점점 순도가 높아지겠금 가지를 치고 내려오면서 분류를 하는 것입니다.
		
순수도 ? 목표변수의 특정범주의 개체들이 포함되어져 있는 정도
			                ( 관심범주 o , 관심범주 x )

부모마디의 순수도에 비해서 자식마디들의 순수도가 증가하도록 자식 마디를 형성해 나가면서
의사결정트리를 만든다.


불순도 ( entropy ) 의 수학공식 ? p 202 페이지



정보 획득량의 수학공식 ?

		분할전 엔트로피 - 분할후 엔트로피


의사결정트리 ppt 참고하세요 !


분할전 엔트로피 : 구매자 10명 / 비구매자 6명

( - 10/16 * log2(10/16) ) + ( - 6/16 * log2(6/16) ) = 0.954434002924965 




분할 후 엔트로피 : 결혼 8명 ( 구매 7명, 비구매 1명 ), 비결혼 8명 ( 구매 3명, 비구매 5명 )

(8/16)* ( ( - 7/8 * log2(7/8) ) + ( - 1/8 * log2(1/8) ) ) +(8/16) *( ( - 3/8 * log2(3/8) ) + ( - 5/8 * log2(5/8) ) ) = 0.748999223062281 
                       결혼o 구매o             결혼o 구매x                                  결혼x 구매o              결혼x 구매x


*정보획득량

0.954434002924965 - 0.748999223062281 = 0.205434779862684 

■ 화장품 구매에 영향을 크게 미치는 변수 ( 컬럼 ) 가 무엇인지 정보획득량을 구하시오 ( skin.csv )  

skin <- read.csv( 'skin.csv', stringsAsFactors = T )
skin

install.packages('FSelector')
library(FSelector)

wg <- information.gain( cupon_react ~. , skin, unit = 'log2' )
wg

※ 설명 : information.gain(라벨컬럼~모든컬럼, 데이터프레임명, unit = 'log2')

               attr_importance
cust_no      0.00000000
gender       0.06798089
age          0.00000000
job          0.03600662
marry        0.18350551
car          0.02487770

### 지방간을 일으키는 원인중에 가장 큰 영향력을 보이는 요인은 무엇인지 정보획득량을
		  구해서 알아내시오 ! ( fatliver2.csv )
		
fat <- read.csv("fatliver2.csv")
wg2 <- information.gain( FATLIVER ~. , fat, unit = 'log2' )
wg2

                attr_importance
AGE         0.032256902
GENDER      0.028650604
DRINK       0.012189492
SMOKING     0.009812076

나이가 가장 큰 영향력을 보인다.

■ 정보획득량을 판다스로 구하기 

예제 1. skin.csv 를 판다스 데이터 프레임으로 만드세요

skin = pd.read_csv("c:\\data\\skin.csv", encoding = 'euckr')

예제 2. skin 데이터 프레임에서 정답 컬럼인 cupon_react 컬럼의 데이터만 x 라는 변수에 담으세요

x = skin.loc[:, 'cupon_react']

예제 3. 구매여부와 결혼유무와의 정보획득량을 구하기 위해서 확률을 구하기 쉽게 판다스의 crosstab 함수를
	     이용해서 아래의 결과를 출력하시오 !
	
ct = pd.crosstab(x , skin['marry'], margins = True)
print(ct)

marry        NO  YES  All
cupon_react              
NO            9    8   17
YES           1   12   13
All            10   20   30




예제 4. ct 데이터 프레임에서 데이터를 추출해서 아래의 before 라는 리스트를 만드시오 !
		before = [ 17/30, 13/30 ]

before = []
ct.loc['All','All']       # 30
ct.loc['NO','All']        # 17
ct.loc['YES','All']        # 13

before.append( ct.loc['NO','All'] / ct.loc['All','All']  )
before.append( ct.loc['YES','All'] / ct.loc['All','All']  )
print(before)

# 선생님답
before = [ ct.loc[i, 'All'] / ct.loc['All','All'] for i in ['NO','YES'] ]

예제 5. 위에서 만든 before 리스트의 확률을 이용해서 분할전 엔트로피를 구하시오 !
	       print(before_entropy)

import numpy as np
before_entropy = np.sum( [ (-1)*(i)*np.log2(i) for i in before ])
print(before_entropy)               # 0.9871377743721863

예제 6. 분할후 엔트로피를 구하기 쉽도록 확률이 아래의 after 리스트에 담기게 하시오
		after = [9/10, 1/10, 8/20, 12/20]

after = [ ct.loc[j, i] / ct.loc['All',i] for i in ['NO','YES'] for j in ['NO','YES'] ]

# [0.9, 0.1, 0.4, 0.6]

예제 7. 위에서 만든 두개의 리스트를 이용해서 분할 후 엔트로피를 출력하시오 !
		marry = [ ct.loc['All',i]/ ct.loc['All','All'] for i in ['NO','YES'] ]
		after = [9/10, 1/10, 8/20, 12/20]           # 결혼 유무 분할 한 후의 확률
		print( after_entropy )



marry = [ ct.loc['All',i]/ ct.loc['All','All'] for i in ['NO','YES'] ]          # [ 10/30, 20/30 ]

after_entropy = 0 
for k in range(0,2):
    after_entropy += np.sum( [ (-1)*(i)*np.log2(i) for i in after[2*k:2*k+2] ])*(marry[k])

print(after_entropy)                  #        0.8036322608328728

예제 8. 화장품 고객 데이터의 결혼여부에 대한 정보 획득량을 출력하시오 !

print( before_entropy - after_entropy )          # 0.18350551353931355

예제 9. R 에서 출력한 결혼 유무에 대한 정보획득량과 결과가 똑같은지 확인하시오 !

skin <- read.csv( 'skin.csv', stringsAsFactors = T )

wg <- information.gain( cupon_react ~. , skin, unit = 'log2' )
wg

               attr_importance
cust_no      0.00000000
gender       0.06798089
age          0.00000000
job          0.03600662
marry        0.18350551
car          0.02487770

( 카페 : 생각해볼문제 )

■ ( R 로 의사결정 트리 모델 구현하기 1 ) 화장품 구매 고객중 구매가 예상되는
     고객은 누구인지 분류하는 의사결정트리 모델 만들기

nrow(skin)           # 30건 밖에 안되므로 29건으로 학습 시키고 1건으로 테스트 합니다.

#1. 의사결정트리에 필요한 패키지를 설치합니다.
#2. 화장품 고객 데이터를 로드합니다.
#3. 화장품 고객 데이터를 훈련 ( 20개 ), 테스트 (10개) 로 나눕니다.
#4. 분류 모델을 만듭니다. 
#5. 훈련한 모델로 테스트 데이터 10개를 예측합니다.
#6. 모델의 성능(정확도)을 확인합니다.
#7. 모델의 성능을 높입니다.


#1. 의사결정트리에 필요한 패키지를 설치합니다.

install.packages("C50")

#2. 화장품 고객 데이터를 로드합니다.

skin <- read.csv("skin.csv", stringsAsFactor = T)
head(skin)

#3. 화장품 고객 데이터를 훈련 ( 20개 ), 테스트 (10개) 로 나눕니다.

library(caret)
set.seed(1)
train_num <- createDataPartition(skin$cupon_react, p = 0.8, list = F)

train_num
length(train_num)          # 25

train_data <- skin[ train_num, ]
test_data <- skin[ -train_num, ]

nrow(train_data)         # 25 ,        컬럼에 문자와 숫자가 섞여있다!! (나이브,knn과 다르네)
nrow(test_data)           # 5

#4. 분류 모델을 만듭니다. 

library(C50)
model <- C5.0( train_data[ , c(-1,-7) ], train_data[ ,7 ] )
				#     ↑                                    ↑
	      # 라벨 제외 훈련데이터    훈련 데이터의 라벨(정답) 

model

※ 설명 : tree : 5 ---------> 가지를 5개 만들었다.
		summary(model)

marry = NO: NO (7)                  # 결혼 안했으면 다 구매 x
marry = YES:
:...car = YES: YES (7/1)             # 결혼을 했는데 차가 있으면 구매 7명인데 1명은 오분류
    car = NO:                                  # 차가 없는 사람중에서
    :...job = NO: NO (4)                 # 직업이 없으면 다 구매 안했음 ( 4명 )
        job = YES:                              # 직업이 있으면
        :...age <= 20: NO (2)            # 나이가 20 이하이면 구매 안했음 ( 2명 )
            age > 20: YES (5)              # 나이가 20보다 높으면 구매했음 ( 5명 )


#5. 훈련한 모델로 테스트 데이터 10개를 예측합니다.

result <- predict(model, test_data[  , c(-1,-7) ]   )
result             # [1] NO  NO  YES YES NO 

#6. 모델의 성능(정확도)을 확인합니다.

sum ( result == test_data[  , 7 ] )            # [1] 3 , 5개 중에서 3개 맞춤
sum( result == test_data[  , 7 ] ) / length(result)       # 0.6

#7. 모델의 성능을 높입니다.

library(C50)

model <- C5.0( train_data[ , c(-1,-7) ], train_data[ ,7 ] , trials = 5)
				#     ↑                                    ↑
	      # 라벨 제외 훈련데이터    훈련 데이터의 라벨(정답) 
※ 훈련 데이터에서 샘플을 추출해서 5개의 의사결정트리를 만들어서 5개의 의사결정 트리 모델이
    다수결에 의해서 훈련 데이터를 분류합니다.

model

Classification Tree
Number of samples: 25 
Number of predictors: 5 

Number of boosting iterations: 5 
Average tree size: 4.6 



summary(model)

Trial	    Decision Tree   
-----	  ----------------  
	  Size      Errors  

   0	     5    1( 4.0%)
   1	     3    5(20.0%)
   2	     5    4(16.0%)
   3	     5    3(12.0%)
   4	     5    2( 8.0%)
boost	          0( 0.0%)   <<


	   (a)   (b)    <-classified as
	  ----  ----
	    14          (a): class NO
	          11    (b): class YES

훈련 데이터에 대해서는 정확도 100 % 의 의사결정트리 모델이 나왔습니다.
trials = 5 를 써서 약한 학습자 5명을 생성해서 5명을 이용해서 강한 학습자를 만들어냄
  ↓
사용자가 직접 알아내야하는 파라미터인 하이퍼 파라미터라고 한다.

result2 <- predict(model, test_data[  , c(-1,-7) ]  )
result2            # [1] NO  NO  YES YES NO

sum( result2 == test_data[ , 7] )             # 3

훈련 데이터에 대해서는 100%의 정확도를 보이는 모델이지만 테스트 데이터는
5개중에 3만 맞췄습니다. 이런 현상을 과대접합 ( overheating ) 이라고 합니다.

30 개는 데이터가 너무 작아서 의사결정트리 + 앙상블을 구현하기 적절하지 않습니다.

### 아이리스 (iris) 꽃을 분류하는 분류 모델을 생성하시오 ! ( 의사결정트리 ) ( iris2.csv )
			정답 컬럼 : Sepcies
	
#1. 의사결정트리에 필요한 패키지를 설치합니다.
install.packages("C50")

#2. 데이터를 로드합니다.
iris <- read.csv("iris2.csv", stringsAsFactor = T)

#3. 훈련 데이터와 테스트 데이터로 데이터를 분리합니다. ( 훈련 8, 테스트 2 )
library(caret)
set.seed(1)
train_num <- createDataPartition(iris$Species, p = 0.8, list = F)

train_num
length(train_num)          # 120

train_data <- iris[ train_num, ]
test_data <- iris[ -train_num, ]

nrow(train_data)         # 120
nrow(test_data)           # 30

#4. 훈련데이터로 의사결정트리 모델을 만듭니다.

library(C50)
model <- C5.0( train_data[ , -5 ], train_data[ ,5 ] )
				#     ↑                                    ↑
	      # 라벨 제외 훈련데이터    훈련 데이터의 라벨(정답) 

model

#5. 훈련한 모델로 테스트 데이터를 예측합니다.

result <- predict(model, test_data[  , -5 ]   )      

#6. 모델의 성능(정확도)을 확인합니다.

sum( result == test_data[  , 5 ] ) / length(result)  # [1] 0.9333333  = 28/30

#7. 모델의 성능을 높입니다.

model2 <- C5.0( train_data[ , -5 ], train_data[ ,5 ] , trials = 10)
				#     ↑                                    ↑
	      # 라벨 제외 훈련데이터    훈련 데이터의 라벨(정답) 

result2 <- predict(model, test_data[  , -5 ]  )         

sum( result2 == test_data[ , 5] )/length(result2)            # [1] 0.9333333

# 8. trials 를 하지 않은 의사결정트리 모델을 시각화 하시오 !

library(C50)

model <- C5.0( train_data[ , -5 ], train_data[ ,5 ] )
plot(model)




### 화장품 구매 여부 분류 모델을 시각화 하시오 !


skin <- read.csv("skin.csv", stringsAsFactor = T)

library(caret)
set.seed(1)
train_num <- createDataPartition(skin$cupon_react, p = 0.8, list = F)

train_data <- skin[ train_num, ]
test_data <- skin[ -train_num, ]

library(C50)
model <- C5.0( train_data[ , c(-1,-7) ], train_data[ ,7 ] )
				#     ↑                                    ↑
	      # 라벨 제외 훈련데이터    훈련 데이터의 라벨(정답) 

result <- predict(model, test_data[  , c(-1,-7) ]   )

plot(model)

▩ ( 의사결정트리 실습 3 ) 은행대출채무를 불이행할 것 같은 고객이 누구인지?

은행에서 의사결정트리 모델을 어떻게 활용을 하는가 ?

은행에 대출을 신청을 했는데 대출 거부를 당했으면 은행에서는 대출 거부된 당사자에게
왜 신청거절되었는지 설명을 해줘야 합니다.
이때 이 모델을 사용합니다.

# 1. 데이터 로드
# 2. 데이터 탐색
# 3. 훈련과 테스트로 데이터를 분리 ( 훈련 데이터 : 9, 테스트 데이터 : 1 )
# 4. 훈련 데이터로 모델을 생성합니다.
# 5. 훈련된 모델을 테스트 데이터를 예측합니다.
# 6. 모델의 성능을 평가합니다.
# 7. 모델의 성능을 개선합니다.

# 1. 데이터 로드

credit <- read.csv("credit.csv", stringsAsFactors = TRUE)
head(credit)
str(credit)

p 207 의 데이터 설명 : 독일의 한 신용기관에서 얻은 대출 정보가 있는 데이터

정답( 라벨 ) 컬럼 : default  ----------> yes : 대출금 상환 안함
								   no : 대출금 상환 함
prop.table( table ( credit$default ) )

# no yes 
# 0.7 0.3

# 30 % 의 해당하는 사람들이 대출금을 상환하지 않고 있음
# 머신러닝 모델을 


# 2. 데이터 탐색

checking_balance : 예금계좌
saving_balance : 적금계좌

amount : 대출금액 ( 250 마르크 ~ 18424 마르크 )
		  100 마르크가 우리나라돈으로 6~7만원

amount 의 데이터를 히스토그램 그래프로 그리시오 

hist(credit$amount)

summary(credit$amount)

#   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
#    250    1366    2320    3271    3972   18424 
# 설명 : 최소 250 마르크 ( 1750 만원 ) ~ 18424 마르크 ( 약 12억 8천만원 ) 사이로 구성되어 있다.

예금 계좌에 입금된 돈의 분포를 확인하시오 !
table( credit$checking_balance )

#    < 0 DM   > 200 DM 1 - 200 DM    unknown 
#          274               63                269         394 
# 1000개의 고객 계좌중에서 200 마르크 이상의 계좌가 63개
# 아예 계좌가 없는 고객이 274 계좌, 1 ~ 200 마르크 사이가 269 명이 있습니다.


# 3. 훈련과 테스트로 데이터를 분리 ( 훈련 데이터 : 9, 테스트 데이터 : 1 )

library(caret)
set.seed(1)          # 어느 자리에서든 동일한 방법으로 훈련과 테스트 데이터를 분리하기 위해서
train_num <- createDataPartition( credit$default, p = 0.9, list = F )
train_num           # 1000개의 데이터 중에 90 %에 해당하는 데이터를 샘플링한 인덱스 번호

train_data <- credit[ train_num,  ]
test_data <- credit[-train_num, ]

nrow(train_data)       # 900
nrow(test_data)        # 100

# 4. 훈련 데이터로 모델을 생성합니다.

library(C50)           # 엔트로피 지수를 이용해서 순수도를 구하고 분류하는 패키지

# 5. 훈련된 모델을 테스트 데이터를 예측합니다.

# 문법 : model <- C5.0( 라벨을 뺀 나머지 데이터, 라벨 컬럼 데이터 )
# ncol( train_data )        # 17


model <- C5.0( train_data[ , -17 ], train_data[, 17] )

※ 설명 : 900 개의 훈련 데이터로 학습한 모델을 생성했습니다.

summary(model)

checking_balance = unknown: no (356/42)
checking_balance in {< 0 DM,> 200 DM,1 - 200 DM}:
:...amount > 8648: yes (31/6)
    amount <= 8648:
    :...credit_history in {perfect,very good}:
        :...housing in {other,rent}: yes (26/3)
        :   housing = own:
        :   :...savings_balance in {> 1000 DM,500 - 1000 DM,
        :       :                   unknown}: no (8/2)
        :       savings_balance = 100 - 500 DM:
        :       :...months_loan_duration <= 16: yes (3)
        :       :   months_loan_duration > 16: no (3)
        :       savings_balance = < 100 DM:
        :       :...age > 33: yes (8)

※ 설명 : checking_balance (예금계좌) 에 200마르크 이상의 돈이 있는 사람들 중에서
		대출금액이 8648 보다 큰 31명의 사람들의 대출금을 상환하지 않았고
		대출금액이 8648 보다 작은 사람들중에 집이 월세인 사람들 26명이 대출금을 상환하지 않았다.
											    집이 자가소유인 8명은 대출금을 상환했습니다.
											    집이 자가소유이면서 적금계좌에 500마르크가 있는 사람들중에
											    적금을 부은 개월수가 16개월 이상이면 대출금을 상환했고,
											     16개월 보다 작으면 상환하지 않았다.
		# 이렇게 분석해서 글을 보여주면 고객이 아주 좋아한다!!

# 문법 : result <- predict ( 모델, 라벨을 뺀 테스트 데이터 )

result <- predict ( model, test_data[ , -17 ] )
table(result)

#  result
#  no yes 
#  81  19 
# 테스트 100명에 대해서 81명은 대출금을 상환했고 19명은 상환하지 않았다고 예측하고 있습니다.

# 6. 모델의 성능을 평가합니다.

# 문법 : table( 실제값, 예측값 )

table( test_data[  ,17 ], result )

#    result         실제
#                       no yes         
#    예측    no  59  11
#                 yes 22   8


※ 100 명중에 67명을 정확하게 예측했으므로 정확도는 67 % 모델입니다.
     채무이행할거 예측했는데 채무를 불이행한 FN 값이 22명이나 되는 모델이므로 성능개선이 필요합니다.

library(gmodels)
CrossTable(test_data[,17], result)


# 7. 모델의 성능을 개선합니다.

의사결정트리의 성능을 높이려면 trials 의 갯수를 조정합니다.
trials 는 의사결정 나무의 갯수를 결정하는 하이퍼 파라미터 입니다.

model2 <- C5.0( train_data[ ,-17 ], train_data[ , 17], trials = 100 )

result2 <- predict( model2, test_data[ , -17] )

table(test_data[ , 17], result2)

#        result2
#        no yes
#  no  60  10
#  yes 20  10

※ trials 를 100으로 지정했더니 FN 값이 2가 줄었습니다.

문제 259. trials 를 200, 300, 400, 500 으로 했을 때 정확도가 어떻게 달라지는지 실험하세여

Error: number of boosting iterations must be between 1 and 100

※ trails 는 1에서 100사이의 수만 가능하다.

문제 . 위에서 만든 모델을 시각화 하시오 ~ ( model 로 시각화하세요 ! )




우리의 관심범주는 yes( 채무 불이행 )
관심범주가 어디인지 항상 확인하고 분석하도록 하자.

위의 모델의 성능을 더 높이려면 ?

		1. 머신러닝 알고리즘을 다른 알고리즘으로 변경합니다.
		2. 기존 의사결정 트리를 그대로 사용하고 분류를 해나가는 기준을 변경
					1. 엔트로피 지수 : 엔트로피 지수로 정보획득량을 구해서 의사결정나무를 구성
					2. 카이제곱 값 : 카이제곱검정으로 정보획득량을 구해서 의사결정나무를 구성
					3. 지니지수 : 지니지수로 정보획득량을 구해서 의사결정나무를 구성

▩ 의사결정 패키지 2가지

	1. C50     : 엔트로피 지수로 정보획득량을 구해서 의사결정나무를 구성
	2. party  : 카이제곱 검정으로 정보획득량을 구해서 의사결정나무를 구성

C50은 trials 를 이용해서 바로 성능개선을 할 수 있는게 장점이고
party는 의사결정트리를 시각화 할 수 있는 장점이 있습니다. ( C50보다 깔끔 )

▩ 독일은행 채무불이행자 예측을 C50 말고 party 패키지를 이용해서 수행

# 1. party 패키지를 설치합니다.
# 2. 독일 은행 데이터를 불러옵니다.
# 3. 훈련 데이터와 테스트 데이터로 분리합니다.
# 4. 모델 생성
# 5. 모델 예측
# 6. 모델 평가
# 7. 모델 시각화

# 1. party 패키지를 설치합니다.

install.packages('party')
library(party)

# 2. 독일 은행 데이터를 불러옵니다.

credit <- read.csv("credit.csv", stringsAsFactors = T )
nrow(credit)      # 1000

# 3. 훈련 데이터와 테스트 데이터로 분리합니다.

train_num <- createDataPartition ( credit$default, p = 0.9 , list = F )

train_data <- credit[ train_num,  ]
test_data <- credit[ -train_num,  ]

nrow(train_data)       # 900
nrow(test_data)        # 100

# 4. 모델 생성

# C5.0 패키지 문법 : model <- C50( 라벨을 뺀 데이터, 라벨 데이터 )
# party 패키지 문법 : model <- ctree( 라벨~. , data = 훈련 데이터 프레임명 )

 model <- ctree( default~. , data = train_data )        # '.' 은 나머지 모든컬럼을 말한다. / 여기서는 라벨을 뺀 모든컬럼

# 5. 모델 예측

result <- predict ( model, test_data[ , -17 ]  )
table(result)

#   result
#  no yes 
#  89  11 

# 6. 모델 평가

table( test_data[  , 17], result )

#      result
#       no yes
#   no  66   4
#  yes 23   7

※ 73 % 의 정확도를 보이는 모델이 생성되었습니다.
    FN 값은 23으로 높다..
    카이제곱으로 해도 크게 개선되지 않았다. 다른 알고리즘 모델을 써야한다.

# 7. 모델 시각화

plot(model)



### iris.csv 데이터를 분류하는 의사결정 트리 모델을 생성하시오 ! ( 책에 나오는 C50 패키지 사용 )

 
iris <- read.csv("iris2.csv", stringsAsFactors = TRUE)

library(caret)
set.seed(1)          # 어느 자리에서든 동일한 방법으로 훈련과 테스트 데이터를 분리하기 위해서
train_num <- createDataPartition( iris$Species, p = 0.9, list = F )
train_num           # 1개의 데이터 중에 90 %에 해당하는 데이터를 샘플링한 인덱스 번호

train_data <- iris[ train_num,  ]
test_data <- iris[-train_num, ]

nrow(train_data)       # 135
nrow(test_data)        # 15

library(C50)           # 엔트로피 지수를 이용해서 순수도를 구하고 분류하는 패키지

model <- C5.0( train_data[ , -5 ], train_data[, 5] )

result <- predict ( model, test_data[ , -5 ] )
table(result)

#  result
#    Iris-setosa Iris-versicolor  Iris-virginica 
#              5               5               5 

table( test_data[  ,5 ], result )


#                     result
#                           Iris-setosa Iris-versicolor Iris-virginica
#  Iris-setosa               5               0              0
#  Iris-versicolor         0               5              0
#  Iris-virginica            0               0              5

# 다 맞췄다;;
※ 설명 : knn 모델에서는 100% 의 정확도가 나오지 않았는데 의사결정트리에서는 100% 의 정확도가 나왔습니다.


▩ R 을 활용해서 의사결정트리 모델 구현.
		1. 화장품 고객
		2. 독일 은행 데이터
		3. iris 데이터


############################################################################################################################

▩ 독버섯 데이터와 판다스를 이용해서 의사결정트리 모델 만들기

# 1 . 데이터를 로드합니다.
# 2. 결측치를 확인합니다.
# 3. 이상치를 확인합니다.
# 4. 명목형 데이터가 있는지 확인하고 숫자형으로 변경합니다.
# 5. 훈련데이터와 테스트 데이터를 나눕니다. ( 9대1 )
# 6. 훈련데이터를 정규화 합니다.
# 7. 테스트 데이터를 정규화 합니다.
# 8. 모델을 생성합니다.
# 9. 모델을 훈련시킵니다.
# 10. 테스트 데이터를 예측합니다.
# 11. 모델을 평가합니다.
# 12. 모델의 성능을 개선합니다.


# 1 . 데이터를 로드합니다.

import pandas as pd
mush = pd.read_csv("c:\\data\\mushrooms.csv")
print ( mush.shape )      # (8124, 23)    # 데이터가 크니까 8대2로 나눠서 해도 되겠다.

# 2. 결측치를 확인합니다.

print( mush.isnull().sum() )        # 0

※ 설명 : 결측치가 있다면 그 컬럼의 중앙값, 최빈값, 평균값 등으로 대치합니다.

# 3. 이상치를 확인합니다.

명목형 데이터이기 때문에 이상치를 확인할 수 없다.

※ 설명 : 이상치가 있다면 이상치를 중앙값, 최빈값, 평균값 등으로 치환합니다.

# 4. 명목형 데이터가 있는지 확인하고 숫자형으로 변경합니다. ( R 과 다른점이 이부분입니다. )

print(mush.head())

mush2 = pd.get_dummies( mush.iloc [ :, 1: ]   )            # 정답 컬럼제외하고 숫자형으로 변경
print( mush2.head() )
x = mush2.to_numpy()                      # 숫자로 변경한 훈련데이터를 numpy array로 변경

y = mush['type'].to_numpy()           # 라벨데이터 생성 / numpy array로 바꿔줌

# 5. 훈련데이터와 테스트 데이터를 나눕니다. ( 8대2 )

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split ( x, y , test_size = 0.2, random_state = 1 )  # x,y 를 numpy array 형태로 받는다.

print(x_train.shape)        # (6499, 117)      / 훈련 데이터
print(x_test.shape)          # (1625, 117)     / 테스트 데이터
print(y_train.shape)         # (6499,)           / 훈련데이터의 라벨
print(y_test.shape)        # (1625,)             / 테스트 데이터의 라벨

# 6. 훈련데이터를 정규화 합니다.

전부 0과 1이므로 정규화 작업을 생략합니다.

# 7. 테스트 데이터를 정규화 합니다.

전부 0과 1이므로 정규화 작업을 생략합니다.

# 8. 의사결정트리 모델을 생성합니다.

from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier ( criterion = 'entropy', max_depth = 5 )       #  max_depth : 가지의 깊이

※ 설명 : criterion 은 entropy 와 gini 가 있습니다.
		max_depth 는 가지의 깊이, 너무 깊으면 훈련 데이터의 정확도는 높은데, 테스트 데이터의 정확도가
					낮아지는 과대적합 현상이 발생합니다.
		둘다 하이퍼 파라미터 ( 직접 알아내야하는 값 )

# 9. 모델을 훈련시킵니다.

model.fit( x_train, y_train )

# 10. 테스트 데이터를 예측합니다.

result = model.predict( x_test )  
print(result)

# 11. 모델을 평가합니다.

print ( sum( result == y_test ) / len(y_test) )      # 1.0 / 성능 개선 필요 없겠다.

## 이원교차표를 한번 보자

from sklearn.metrics import confusion_matrix

print ( confusion_matrix( y_test, result ) )             # 순서는 ( 실제, 예측 )
   
# [[820   0]
#  [  0 805]]

### 위의 모델을 다시 만드는데 이번에는 entropy 지수가 아니라 gini 지수로 해보세요

import pandas as pd
mush = pd.read_csv("c:\\data\\mushrooms.csv")

mush2 = pd.get_dummies( mush.iloc [ :, 1: ]   )            # 정답 컬럼제외하고 숫자형으로 변경

x = mush2.to_numpy()                      # 숫자로 변경한 훈련데이터를 numpy array로 변경
y = mush['type'].to_numpy()           # 라벨데이터 생성 / numpy array로 바꿔줌

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split ( x, y , test_size = 0.2, random_state = 1 )  # x,y 를 numpy array 형태로 받는다.

from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier ( criterion = 'gini', max_depth = 5 )       #  max_depth : 가지의 깊이
model.fit( x_train, y_train )

result = model.predict( x_test )  

print ( sum( result == y_test ) / len(y_test) )      # 0.9987692307692307

from sklearn.metrics import confusion_matrix
print ( confusion_matrix( y_test, result ) )             # 순서는 ( 실제, 예측 )

# [[820   0]
#  [  2 803]]

# gini 지수 테스트 한 결과 entropy 로 했을 때 보다 정확도가 떨어졌다. fn 값이 2가 증가 했습니다.

### iris 데이터로 판다스를 활용한 의사결정트리 모델을 생성하시오 ~

# 1. 데이터를 로드합니다.
import pandas as pd
iris = pd.read_csv("c:\\data\\iris2.csv")

# 2. 결측치를 확인합니다.
print(iris.isnull().sum())

# 3. 이상치를 확인합니다.

def outlier_value(x):

    for i in list(x.describe ().columns):             # x.columns[x.dtypes =='float64']
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        upper_bound = Q3 + (IQR*1.5)
        lower_bound = Q1 - (IQR*1.5)
        a = x.loc[ (x[i] > upper_bound ) | ( x[i]< lower_bound ) , i ].count()
        b = i
        print( '{0:<10} : {1:>5} 건'.format(  b , a  ) )
        print(x.loc[ (x[i] > upper_bound ) | ( x[i]< lower_bound ) , i ])     # 이상치 행 / 이상치 값을 출력해줌

outlier_value(iris)

결과 :
Sepal.Length :     0 건
Sepal.Width :     4 건
15    4.4
32    4.1
33    4.2
60    2.0
Petal.Length :     0 건
Petal.Width :     0 건

# Sepal.Width 이상치 4개를 Sepal.Width의 평균값으로 치환해보자

mean = iris[ 'Sepal.Width' ].mean()              # Sepal.Width 컬럼의 평균값을 mean에 담고

iris.loc [ iris['Sepal.Width'].isin([4.4, 4.1, 4.2, 2.0]), 'Sepal.Width' ] = mean       # 이상치를 평균값으로 치환

outlier_value(iris)         # 다시 확인

결과 :
Sepal.Length :     0 건
Sepal.Width :     0 건
Petal.Length :     0 건
Petal.Width :     0 건


# 4. 명목형 데이터를 숫자로 변경합니다.

print( iris.info() )
라벨 데이터 빼고 모두 숫자형 입니다.

# 5. 훈련 데이터와 테스트 데이터를 나눕니다.

x = iris.iloc [ :, :-1].to_numpy()         # 정답 컬럼을 제외한 데이터를 numpy array로 변경
y = iris['Species'].to_numpy()           # 라벨데이터 생성 / numpy array로 바꿔줌

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split ( x, y , test_size = 0.2, random_state = 1 )  # x,y 를 numpy array 형태로 받는다.

print(x_train.shape)        # (120, 4)      / 훈련 데이터
print(x_test.shape)          # (30, 4)     / 테스트 데이터
print(y_train.shape)         # (120,)           / 훈련데이터의 라벨
print(y_test.shape)        # (30,)             / 테스트 데이터의 라벨

# 6. 훈련 데이터를 정규화 합니다.

from sklearn.preprocessing import MinMaxScaler           # standardscaler 보다 minmax가 더 잘나오더라 보통

scaler = MinMaxScaler()                                    # 정규화 모델생성
scaler.fit( x_train )                                              # 훈련데이터를 가지고 정규화 계산
x_train2 = scaler.transform( x_train )           # 계산된 내용으로 데이터를 변환해서 x_train2에 담는다.

# 7. 테스트 데이터를 정규화 합니다.

scaler = MinMaxScaler()                                    # 정규화 모델생성
scaler.fit( x_test )                                              # 훈련데이터를 가지고 정규화 계산
x_test2 = scaler.transform( x_test )           # 계산된 내용으로 데이터를 변환해서 x_test2에 담는다.

# 8. 모델 생성

from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier ( criterion = 'entropy', max_depth = 4 )

# 9. 모델 훈련

model.fit( x_train2, y_train )

# 10. 모델 예측

result = model.predict( x_test2 )  

# 11. 모델 평가

print ( sum( result == y_test ) / len(y_test) )      #  0.9333333333333333

from sklearn.metrics import confusion_matrix
print ( confusion_matrix( y_test, result ) )             # 순서는 ( 실제, 예측 )

# [[11  0  0]
#  [ 0 11  2]
#  [ 0  0  6]]


※ 모델 만드는 순서 정리

# 1. 데이터 로드
# 2. 결측치 확인
# 3. 이상치 확인
# 4. 명목형 ----> 숫자형
# 5. 훈련과 테스트로 데이터를 분리
# 6. 훈련데이터 정규화
# 7. 테스트 데이터 정규화
# 8. 모델 생성
# 9. 모델 훈련
# 10. 모델 예측
# 11. 모델 평가 

### 위의 결과는 이상치를 평균값으로 치환하고 0.93333 의 정확도가 나왔는데
		  이번에는 그냥 이상치를 그냥 그대로 두고 정확도가 어떻게 나오는지 실험하시오 !
		
import pandas as pd
iris = pd.read_csv("c:\\data\\iris2.csv")

x = iris.iloc [ :, :-1].to_numpy()         # 정답 컬럼을 제외한 데이터를 numpy array로 변경
y = iris['Species'].to_numpy()           # 라벨데이터 생성 / numpy array로 바꿔줌

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split ( x, y , test_size = 0.2, random_state = 1 )  # x,y 를 numpy array 형태로 받는다.

from sklearn.preprocessing import MinMaxScaler           # standardscaler 보다 minmax가 더 잘나오더라 보통

scaler = MinMaxScaler()                                    # 정규화 모델생성
scaler.fit( x_train )                                              # 훈련데이터를 가지고 정규화 계산
x_train2 = scaler.transform( x_train )           # 계산된 내용으로 데이터를 변환해서 x_train2에 담는다.

scaler = MinMaxScaler()                                    # 정규화 모델생성
scaler.fit( x_test )                                              # 훈련데이터를 가지고 정규화 계산
x_test2 = scaler.transform( x_test )           # 계산된 내용으로 데이터를 변환해서 x_test2에 담는다.

from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier ( criterion = 'entropy', max_depth = 4 )

model.fit( x_train2, y_train )

result = model.predict( x_test2 )  

print ( sum( result == y_test ) / len(y_test) )      #  0.9

from sklearn.metrics import confusion_matrix
print ( confusion_matrix( y_test, result ) )             # 순서는 ( 실제, 예측 )


# [[11  0  0]
#  [ 0 10  3]
#  [ 0  0  6]]

※ 설명 : 정확도가 0.933333 에서 0.9로 떨어짐
		이상치를 제거했을때는 0.9333 이 나왔으므로 이상치를 제거한 경우가 더 성능이 좋게나왔다.
		
### 다시 이상치를 제거한 코드로 구현하는데 이번에는 정규화를 할때
		  훈련을 할때 계산했던 방법으로 훈련 데이터 뿐만 아니라 테스트 데이터로 transform 되게 하시오 !

import pandas as pd
iris = pd.read_csv("c:\\data\\iris2.csv")

mean = iris[ 'Sepal.Width' ].mean()              # Sepal.Width 컬럼의 평균값을 mean에 담고
iris.loc [ iris['Sepal.Width'].isin([4.4, 4.1, 4.2, 2.0]), 'Sepal.Width' ] = mean       # 이상치를 평균값으로 치환

x = iris.iloc [ :, :-1].to_numpy()         # 정답 컬럼을 제외한 데이터를 numpy array로 변경
y = iris['Species'].to_numpy()           # 라벨데이터 생성 / numpy array로 바꿔줌

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split ( x, y , test_size = 0.2, random_state = 1 )  # x,y 를 numpy array 형태로 받는다.

from sklearn.preprocessing import MinMaxScaler           # standardscaler 보다 minmax가 더 잘나오더라 보통

scaler = MinMaxScaler()                                    # 정규화 모델생성
scaler.fit( x_train )                                              # 훈련데이터를 가지고 정규화 계산
x_train2 = scaler.transform( x_train )           # 계산된 내용으로 데이터를 변환해서 x_train2에 담는다.

# scaler = MinMaxScaler()                                    # 정규화 모델생성
# scaler.fit( x_test )                                              # 훈련데이터를 가지고 정규화 계산
x_test2 = scaler.transform( x_test )           # 계산된 내용으로 데이터를 변환해서 x_test2에 담는다.

from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier ( criterion = 'entropy', max_depth = 4 )

model.fit( x_train2, y_train )

result = model.predict( x_test2 )  

print ( sum( result == y_test ) / len(y_test) )      #  0.9666666666666667

from sklearn.metrics import confusion_matrix
print ( confusion_matrix( y_test, result ) ) 

※ 설명 : 0.9의 정확도가 출력되었습니다. 이상치를 제거했을때는 0.9333 이 나왔으므로
		이상치를 제거한 경우가 더 성능이 좋게나왔습니다. 

		이번에는 훈련 때 사용했던 정규화 계산방법으로 테스트 데이터를 정규화 했더니
		정확도가 0.9666666666666667 으로 더 향상되었습니다. 


### 위에서 실험했을때는 정규화를 했는데 이번에는 표준화를 해서 테스트 하세요 ~
		정규화 : 데이터를 0 ~ 1 사이로 변환
		표준화 : 평균을 0 으로 두고 평균 중심으로 데이터를 변환한다 ( -1 ~ 1 )

		정규화 : from sklearn.preprocessing import MinMaxScaler
		표준화 : from sklearn.preprocessing import StandardScaler

from sklearn.preprocessing import StandardScaler   

scaler = StandardScaler()                                    # 표준화 모델생성

위의 문제에서 MinMaxScaler 를 StandardScaler로 바꾸면 된다.

0.9666666666666667
[[11  0  0]
 [ 0 12  1]
 [ 0  0  6]]

※ 설명 : 정규화로 했을때와 표준화로 했을때의 정확도 차이는 없었습니다.

### 위의 실험에서 이상치를 평균값으로 치환했는데 이번에는 중앙값/최빈값으로 치환하고,
		  정확도를 확인하시오 

medi = iris[ 'Sepal.Width' ].median()              # Sepal.Width 컬럼의 중앙값을 mean에 담고
iris.loc [ iris['Sepal.Width'].isin([4.4, 4.1, 4.2, 2.0]), 'Sepal.Width' ] = medi       # 이상치를 중앙값으로 치환

0.9666666666666667
[[11  0  0]
 [ 0 12  1]
 [ 0  0  6]]

mode = iris[ 'Sepal.Width' ].mode()[0]              # Sepal.Width 컬럼의 최빈값을 mean에 담고
iris.loc [ iris['Sepal.Width'].isin([4.4, 4.1, 4.2, 2.0]), 'Sepal.Width' ] = mode       # 이상치를 최빈값으로 치환

0.9666666666666667
[[11  0  0]
 [ 0 12  1]
 [ 0  0  6]]

### 위의 iris 분류 모델의 의사결정트리의 분류 수학공식을 entropy 가 아니라 
					   gini 로 변경하고 결과를 확인하시오 !

import pandas as pd
iris = pd.read_csv("c:\\data\\iris2.csv")

mean = iris[ 'Sepal.Width' ].mean()              # Sepal.Width 컬럼의 평균값을 mean에 담고
iris.loc [ iris['Sepal.Width'].isin([4.4, 4.1, 4.2, 2.0]), 'Sepal.Width' ] = mean       # 이상치를 평균값으로 치환

x = iris.iloc [ :, :-1].to_numpy()         # 정답 컬럼을 제외한 데이터를 numpy array로 변경
y = iris['Species'].to_numpy()           # 라벨데이터 생성 / numpy array로 바꿔줌

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split ( x, y , test_size = 0.2, random_state = 1 )  # x,y 를 numpy array 형태로 받는다.

from sklearn.preprocessing import MinMaxScaler           # standardscaler 보다 minmax가 더 잘나오더라 보통

scaler = MinMaxScaler()                                    # 정규화 모델생성
scaler.fit( x_train )                                              # 훈련데이터를 가지고 정규화 계산
x_train2 = scaler.transform( x_train )           # 계산된 내용으로 데이터를 변환해서 x_train2에 담는다.

# scaler = MinMaxScaler()                                    # 정규화 모델생성
# scaler.fit( x_test )                                              # 훈련데이터를 가지고 정규화 계산
x_test2 = scaler.transform( x_test )           # 계산된 내용으로 데이터를 변환해서 x_test2에 담는다.

from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier ( criterion = 'entropy', max_depth = 4  )

model.fit( x_train2, y_train )

result = model.predict( x_test2 )  

print ( sum( result == y_test ) / len(y_test) )      #  0.9666666666666667

from sklearn.metrics import confusion_matrix
print ( confusion_matrix( y_test, result ) ) 

결과 :

0.9666666666666667
[[11  0  0]
 [ 0 12  1]
 [ 0  0  6]]

※ gini로 해도 결과는 똑같다.

▩ 판다스로 독일 은행 채무 불이행자를 예측하는 기계학습 모델 만들기

# 1. 데이터를 로드

import pandas as pd
credit = pd.read_csv("c:\\data\\credit.csv")
print (credit.head() )
print( credit.shape )          # 1000,17

# 2. 데이터 탐색 ( 결측치 확인 )

print(credit.isnull().sum())

# 3. 데이터 탐색 ( 이상치 확인 )          ★

print(credit.info())


def outlier_value(x):

    for i in list(x.describe ().columns):             # x.columns[x.dtypes =='float64']
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        upper_bound = Q3 + (IQR*1.5)
        lower_bound = Q1 - (IQR*1.5)
        a = x.loc[ (x[i] > upper_bound ) | ( x[i]< lower_bound ) , i ].count()
        b = i
        print( '{0:<10} : {1:>5} 건'.format(  b , a  ) )
        # print(x.loc[ (x[i] > upper_bound ) | ( x[i]< lower_bound ) , i ])     # 이상치 행 / 이상치 값을 출력해줌

outlier_value(credit)

결과:
months_loan_duration :    70 건
amount     :    72 건
percent_of_income :     0 건
years_at_residence :     0 건
age        :    23 건
existing_loans_count :     6 건
dependents :   155 건

이상치가 보이는 컬럼들은 일단 두고 모델 생서후에 조정해보기로 함


# 4. 데이터 탐색 ( 명목형 데이터 )

print( credit.info() )

 0   checking_balance      1000 non-null   object
 2   credit_history        1000 non-null   object
 3   purpose               1000 non-null   object
 5   savings_balance       1000 non-null   object
 6   employment_duration   1000 non-null   object
 10  other_credit          1000 non-null   object
 11  housing               1000 non-null   object
 13  job                   1000 non-null   object
 15  phone                 1000 non-null   object
 16  default               1000 non-null   object            # 정답 컬럼

credit2 = pd.get_dummies( credit.iloc[: , :-1] )
print( credit2.info() )

# 5. 훈련 데이터와 테스트 데이터를 분리

x = credit2.to_numpy()                         # 학습시킬 데이터 생성
y = credit.iloc[ : , -1 ].to_numpy()        # 정답 데이터 생성

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split( x,y, test_size = 0.1, random_state = 1 )

print(x_train.shape)         # (900,44)
print(x_test.shape)          # (100,44)
print(y_train.shape)        # (900,)
print(y_test.shape)         # (100,)

# 6. 훈련 데이터로 정규화 계산

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(x_train)              # 훈련데이터 min/max 정규화 계산


# 7. 계산된 내용으로 훈련데이터를 변형       ★

x_train2 = scaler.transform(x_train)


# 7-2. 계산된 내용으로 테스트 데이터를 변형

x_test2 = scaler.transform(x_test)

print( x_train2.max() , x_train2.min() )           # 1.0 , 0.0
print( x_test2.max(), x_test2.min() )             # 1.2142857142857142  ,  0.0  / 훈련데이터로 훈련했더니 1보다 큰값 나옴

# 8. 모델 생성

from sklearn.tree import DecisionTreeClassifier
model =  DecisionTreeClassifier( criterion = 'entropy', max_depth = 5 )

# 9. 모델 훈련

# 모델명.fit( 훈련데이터, 정답 데이터 )

model.fit(x_train2, y_train)

# 10. 모델 예측

result = model.predict( x_test2 )
result

# 11. 모델 평가

print ( sum( result == y_test ) / len(y_test) )     # 0.76

from sklearn.metrics import confusion_matrix
a = confusion_matrix( y_test, result )
print(a)

# [[58 12]
#  [12 18]]

# 12. 모델 개선

	1. 의사결정트리 나무의 가지수인 max_depth 를 늘리는 방법
	2. 의사결정트리 + 앙상블 기법 = Random forest 로 모델을 변경합니다.
	3. 이상치를 보이는 컬럼의 데이터를 평균값으로 변경
	4. 도메인 지식이 있다면 파생변수를 생성

문제 267. max_depth를 5에서 7로 늘리고 정확도를 확인하시오 ~

# 8. 모델 생성

from sklearn.tree import DecisionTreeClassifier
model =  DecisionTreeClassifier( criterion = 'entropy', max_depth =7  )

# 9. 모델 훈련

# 모델명.fit( 훈련데이터, 정답 데이터 )

model.fit(x_train2, y_train)

# 10. 모델 예측

result = model.predict( x_test2 )
result

# 11. 모델 평가

print ( sum( result == y_test ) / len(y_test) )     # 0.77       # 0.01 올라감

from sklearn.metrics import confusion_matrix
a = confusion_matrix( y_test, result )
print(a)

# [[60 10]
#  [13 17]]

※ max_depth 를 5에서 7로 늘렸더니 정확도가 0.01 올라갔고 FN 값은 오히려 1 증가했습니다.

### 의사결정트리의 앙상블 기법을 추가한 RandomForest 로 모델을 변경하고 실험하시오 !
		앙상블 ? 여러 약한 학습자들을 모아서 하나의 강한 학습자로 만드는 기법 ( 11장 )

기존 모델 : from sklearn.tree import DecisionTreeClassifier
		      model =  DecisionTreeClassifier( criterion = 'entropy', max_depth =7  )
						↓
변경 모델 : from sklearn.ensemble import RandomForestClassifier
		      model = RandomForestClassifier( random_state = 1, n_estimators = 100 )      # seed값 = 1, 나무개수 = 100

※ 딱히 큰 차이가 없다..

### 와인의 품질을 분류하는 머신러닝 모델을 생성하시오 ! ( wine.csv )

# 1. 데이터를 로드

import pandas as pd
wine = pd.read_csv("c:\\data\\wine.csv")
print( wine.shape )          # 178,14

# 2. 데이터 탐색 ( 결측치 확인 )

print(wine.isnull().sum())

# 3. 데이터 탐색 ( 이상치 확인 )          ★

print(wine.info())

def outlier_value(x):

    for i in list(x.describe ().columns):             # x.columns[x.dtypes =='float64']
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        upper_bound = Q3 + (IQR*1.5)
        lower_bound = Q1 - (IQR*1.5)
        a = x.loc[ (x[i] > upper_bound ) | ( x[i]< lower_bound ) , i ].count()
        b = i
        print( '{0:<10} : {1:>5} 건'.format(  b , a  ) )
        # print(x.loc[ (x[i] > upper_bound ) | ( x[i]< lower_bound ) , i ])     # 이상치 행 / 이상치 값을 출력해줌

outlier_value(wine)

결과:
Alcohol    :     0 건
Malic      :     3 건
Ash        :     3 건
Alcalinity :     4 건
Magnesium  :     4 건
Phenols    :     0 건
Flavanoids :     0 건
Nonflavanoids :     0 건
Proanthocyanins :     2 건
Color      :     4 건
Hue        :     1 건
Dilution   :     0 건
Proline    :     0 건

이상치가 보이는 컬럼들은 일단 두고 모델 생성후에 조정해보기로 함


# 4. 데이터 탐색 ( 명목형 데이터 )

print( wine.info() )

 0   Type             178 non-null    object           # 정답 컬럼
 1   Alcohol          178 non-null    float64
 2   Malic            178 non-null    float64
 3   Ash              178 non-null    float64
 4   Alcalinity       178 non-null    float64
 5   Magnesium        178 non-null    int64  
 6   Phenols          178 non-null    float64
 7   Flavanoids       178 non-null    float64
 8   Nonflavanoids    178 non-null    float64
 9   Proanthocyanins  178 non-null    float64
 10  Color            178 non-null    float64
 11  Hue              178 non-null    float64
 12  Dilution         178 non-null    float64
 13  Proline          178 non-null    int64


# 5. 훈련 데이터와 테스트 데이터를 분리

x = wine.iloc[: , 1:].to_numpy()                         # 학습시킬 데이터 생성
y = wine.iloc[ : , 0 ].to_numpy()        # 정답 데이터 생성

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split( x,y, test_size = 0.1, random_state = 1 )

print(x_train.shape)         # (160,13)
print(x_test.shape)          # (18,13)
print(y_train.shape)        # (160,)
print(y_test.shape)         # (18,)

# 6. 훈련 데이터로 정규화 계산

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(x_train)              # 훈련데이터 min/max 정규화 계산


# 7. 계산된 내용으로 훈련데이터를 변형       ★

x_train2 = scaler.transform(x_train)


# 7-2. 계산된 내용으로 테스트 데이터를 변형

x_test2 = scaler.transform(x_test)

print( x_train2.max() , x_train2.min() )           # 1.0 , 0.0
print( x_test2.max(), x_test2.min() )             # 0.9051355206847361 -0.0326530612244898

# 8. 모델 생성

from sklearn.tree import DecisionTreeClassifier
model =  DecisionTreeClassifier( criterion = 'entropy', max_depth = 5, random_state = 2 )

# 9. 모델 훈련

# 모델명.fit( 훈련데이터, 정답 데이터 )

model.fit(x_train2, y_train)

# 10. 모델 예측

result = model.predict( x_test2 )
result

# 11. 모델 평가

print ( sum( result == y_test ) / len(y_test) )     # 0.9444444444444

from sklearn.metrics import confusion_matrix
a = confusion_matrix( y_test, result )
print(a)

# [[7 0 0]
#  [0 7 0]
#  [0 1 3]]

# 12. 모델 성능 개선

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier( random_state = 1, n_estimators = 100 )      # seed값 = 1, 나무개수 = 100

# 1.0
# [[7 0 0]
#  [0 7 0]
#  [0 0 4]]

▩ 롯데 백화점 고객 데이터를 가지고 고객의 성별을 예측하는 머신러닝 모델 만들기

# 1. 데이터를 로드

df_x = pd.read_csv("c:\\data\\X_train.csv", encoding = 'euckr')
df_y = pd.read_csv("c:\\data\\y_train.csv")

※ 설명 : 성별 0 = 여자, 1 = 남자

# 2. 결측치를 확인

print ( df_x.isnull().sum() )

※ 설명 : 환불금액 컬럼이 3500건 중에 2295 건이 결측치 이므로 컬럼을 삭제합니다.

df_x.drop(['환불금액'],axis =1, inplace = True)
print ( df_x.isnull().sum() )

# 3. 이상치 확인

print(df_x.info())

def outlier_value(x):

    for i in list(x.describe ().columns):             # x.columns[x.dtypes =='float64']
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        upper_bound = Q3 + (IQR*3)
        lower_bound = Q1 - (IQR*3)
        a = x.loc[ (x[i] > upper_bound ) | ( x[i]< lower_bound ) , i ].count()
        b = i
        print( '{0:<10} : {1:>5} 건'.format(  b , a  ) )
        # print(x.loc[ (x[i] > upper_bound ) | ( x[i]< lower_bound ) , i ])     # 이상치 행 / 이상치 값을 출력해줌

outlier_value( df_x )

결과 : ( 이상치가 많아서 IQR*3으로 실행했다. 그럼에도 많다.. )
cust_id    :     0 건
총구매액       :   164 건
최대구매액      :   149 건
내점일수       :    92 건
내점당구매건수    :    69 건
주말방문비율     :     0 건
구매주기       :    57 건

위에서 보이는 이상치값을 모델 평가할 때 조정해보겠습니다.

# 4. 명목형 변수 확인

print(df_x.info())

df_x2 = pd.get_dummies(df_x)
print(df_x2.head())

# 5. 훈련 데이터와 테스트 데이터 분리

x = df_x2.iloc[:, 1:].to_numpy()
y = df_y['gender'].to_numpy()

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 1)

print(x_train.shape)       # ( 2800,72 )
print(x_test.shape)         # ( 700, 72 )
print(y_train.shape)        # ( 2800, )
print(y_test.shape)         # ( 700, )

# 6. 정규화

from sklearn.preprocessing import MinMaxScaler

scaler.fit(x_train)
x_train2 = scaler.transform(x_train)
x_test2 = scaler.transform(x_test)

# 7. 모델생성

from sklearn.tree import DecisionTreeClassifier

model = DecisionTreeClassifier(criterion = 'entropy', max_depth = 5)

# 8. 모델훈련

model.fit(x_train2, y_train)

# 9. 모델예측

result = model.predict(x_test2)

# 10. 모델 평가

print ( sum(result == y_test) / len(y_test) )        #  0.6257142857142857

# 11. 모델 성능 개선

문제 270. 위의 성별 예측 모델을 랜덤포레스트 ( 의사결정트리 + 앙상블 ) 로 변경해서 실험하시오 !

# 7. 모델생성

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier( random_state = 1, n_estimators = 100 )

# 8. 모델훈련

model.fit(x_train2, y_train)

# 9. 모델예측

result = model.predict(x_test2)

# 10. 모델 평가

print ( sum(result == y_test) / len(y_test) )        #  0.6514285714285715

※ 모델의 성능을 높이는 방법?
	1. 이상치를 제거하거나 다른 값으로 치환합니다.
	2. 파생변수를 생성해야합니다.


### 총구매액의 이상치를 평균값으로 치환하고 모델의 정확도를 보는 실험을 하시오 !

cust_id    :     0 건
총구매액       :   164 건
최대구매액      :   149 건
내점일수       :    92 건
내점당구매건수    :    69 건
주말방문비율     :     0 건
구매주기       :    57 건


# 선생님이 한거






### 파생변수를 추가해서 모델의 정확도를 더 올리시오 !
		  남성과 여성을 예측하는것이므로 주구매상품에 남성이 포함되어져 있으면 1 아니면 0이
		  출력되는 파생변수를 추가하고 정확도를 확인하세요 !
		
성별분류 랜덤포르스트 전체코드에서 맨위에 데이터 로드 다음에 아래의 코드를 추가합니다.

print ( df_x['주구매상품'].unique())
mask = df_x['주구매상품'].apply(lambda x:'남성' in x)        # True or False
mask2 = mask.astype(int)
df_x['man_product'] = mask2

# 추가한 코드

df_x = pd.read_csv("c:\\data\\X_train.csv", encoding = 'euckr')
df_y = pd.read_csv("c:\\data\\y_train.csv")

mask = df_x['주구매상품'].apply(lambda x:'남성' in x)        # True or False
mask2 = mask.astype(int)
df_x['man_product'] = mask2

df_x.drop(['환불금액'],axis =1, inplace = True)

df_x2 = pd.get_dummies(df_x)

x = df_x2.iloc[:, 1:].to_numpy()
y = df_y['gender'].to_numpy()

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 1)

from sklearn.preprocessing import MinMaxScaler

scaler.fit(x_train)
x_train2 = scaler.transform(x_train)
x_test2 = scaler.transform(x_test)

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier( random_state = 1, n_estimators = 100 )

model.fit(x_train2, y_train)

result = model.predict(x_test2)

print ( sum(result == y_test) / len(y_test) )        #  0.6457142857142857

※ 설명 : 위의 파생변수를 추가했더니 아주 살짝 정확도가 올라갔습니다.

### 타이타닉 데이터의 생존자를 예측하는 분류 모델을 생성하시오 !
		  ( 데이터셋 : titanic.csv , label = survived 컬럼 / 1 : 생존자 / 0 : 사망자 )


# 1. 데이터를 로드
tat = pd.read_csv("c:\\data\\titanic.csv")
print( tat.head() )

# 필요한 컬럼만 선별합니다.
tat2 = tat.iloc[:, 1:10]
print(tat2.info())

# 2. 결측치 확인
print(tat2.isnull().sum())

결과 :

age         177
embarked      2

# 나이의 결측치를 나이의 평균값으로 치환하세요 ~

mean = tat2['age'].mean()
tat2['age'].fillna(mean , inplace = True)

print(tat2.isnull().sum())

결과:
embarked      2

# embarked 의 결측치를 나이의 평균값으로 치환하세요 ~

mode = tat2['embarked'].mode()[0]
tat2['embarked'].fillna(mode, inplace = True)
print(tat2.isnull().sum())               # 다 없앰


# 3. 이상치 확인

def outlier_value(x):

    for i in list(x.describe ().columns):             # x.columns[x.dtypes =='float64']
        Q1 = x[i].quantile(0.25)
        Q3 = x[i].quantile(0.75)
        IQR = Q3 - Q1
        upper_bound = Q3 + (IQR*1.5)
        lower_bound = Q1 - (IQR*1.5)
        a = x.loc[ (x[i] > upper_bound ) | ( x[i]< lower_bound ) , i ].count()
        b = i
        print( '{0:<10} : {1:>5} 건'.format(  b , a  ) )
        # print(x.loc[ (x[i] > upper_bound ) | ( x[i]< lower_bound ) , i ])     # 이상치 행 / 이상치 값을 출력해줌

outlier_value( tat2 )

결과 :

survived   :     0 건
pclass     :     0 건
age        :    66 건
sibsp      :    46 건
parch      :   213 건
fare       :   116 건


# 4. 명목형 데이터 확인

tat3 = pd.get_dummies(tat2)

x = tat3.iloc[:, 1:].to_numpy()
y = tat3.iloc[:,0].to_numpy()

# 5. 훈련 데이터와 테스트 데이터를 분리


from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 1)


# 6. 정규화

from sklearn.preprocessing import MinMaxScaler

scaler.fit(x_train)
x_train2 = scaler.transform(x_train)
x_test2 = scaler.transform(x_test)

# 7. 모델 생성

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier( random_state = 1, n_estimators = 100 )

# 8. 모델 훈련

model.fit(x_train2, y_train)

# 9. 모델 예측

result = model.predict(x_test2)

# 10. 모델 평가

print ( sum(result == y_test) / len(y_test) )        #  0.7821229050279329

문제 274. 위의 타이타닉 생존자 예측 기계학습 모델의 성능을 더 올리시오
		  ( 여자와 아이먼저~ / 아이 : 10살이하 / )


mask = ( tat['age'] <= 10 ) |(tat2['sex'] == 'female')
mask2 = mask.astype(int)
tat2['child_female'] = mask2

# 위 코드 추가해서

tat = pd.read_csv("c:\\data\\titanic.csv")

tat2 = tat.iloc[:, 1:10]
mask = ( tat2['age'] <= 10 ) |(tat2['sex'] == 'female')
mask2 = mask.astype(int)
tat2['child_female'] = mask2

# 나이의 결측치를 나이의 평균값으로 치환하세요 ~

mean = tat2['age'].mean()
tat2['age'].fillna(mean , inplace = True)

# embarked 의 결측치를 나이의 평균값으로 치환하세요 ~

mode = tat2['embarked'].mode()[0]
tat2['embarked'].fillna(mode, inplace = True)

tat3 = pd.get_dummies(tat2)

x = tat3.iloc[:, 1:].to_numpy()
y = tat3.iloc[:,0].to_numpy()

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 1)


from sklearn.preprocessing import MinMaxScaler

scaler.fit(x_train)
x_train2 = scaler.transform(x_train)
x_test2 = scaler.transform(x_test)

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier( random_state = 1, n_estimators = 100 )

model.fit(x_train2, y_train)

result = model.predict(x_test2)

print ( sum(result == y_test) / len(y_test) )        #  0.7932960893854749

문제 275. 위의 머신러닝 모델을 의사결정트리 모델의 정확도는 어떻게 되는지 확인하시오 !.

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier( random_state = 1, n_estimators = 100 )
				↓
from sklearn.tree import DecisionTreeClassifier
model =  DecisionTreeClassifier( criterion = 'entropy', max_depth = 5 , random_state = 3)

# 0.8044692737430168

### 위의 기계학습 모델을 naivebayes 모델로 변경하고 정확도를 확인하세요 !

from sklearn.naive_bayes import GaussianNB

model = GaussianNB(var_smoothing = 0.04)

# 0.776536312849162

### 위의 머신러닝 모델의 knn 정확도 까지 완성시키시오
			랜럼포레스트 : 0.79
			의사결정트리 : 0.81
			나이브베이즈 : 0.77
			knn 모델 : ?

tat = pd.read_csv("c:\\data\\titanic.csv")

tat2 = tat.iloc[:, 1:10]
mask = ( tat2['age'] <= 10 ) |(tat2['sex'] == 'female')
mask2 = mask.astype(int)
tat2['child_female'] = mask2

# 나이의 결측치를 나이의 평균값으로 치환하세요 ~

mean = tat2['age'].mean()
tat2['age'].fillna(mean , inplace = True)

# embarked 의 결측치를 나이의 평균값으로 치환하세요 ~

mode = tat2['embarked'].mode()[0]
tat2['embarked'].fillna(mode, inplace = True)

tat3 = pd.get_dummies(tat2)

x = tat3.iloc[:, 1:].to_numpy()
y = tat3.iloc[:,0].to_numpy()

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 1)


from sklearn.preprocessing import MinMaxScaler

scaler.fit(x_train)
x_train2 = scaler.transform(x_train)
x_test2 = scaler.transform(x_test)

a =[]

for z in range(1,100):
    from sklearn.neighbors import KNeighborsClassifier

    model = KNeighborsClassifier( n_neighbors = z  )                 # knn 모델생성, k = 넣어준 모델

    model.fit(x_train2, y_train)

    result = model.predict(x_test2)

    a.append( sum(result == y_test) / len(y_test) )

print(max(a),a.index(max(a))+1 )                  #  0.8044692737430168 ,  3

※ k 값이 3 일때 0.80446으로 가장 높은 정확도를 나타냈다.







▩ LabelEncoder 를 사용해서 명목형 데이터를 숫자형으로 변환하기

명목형 데이터를 숫자로 변환하는 방법 2가지

		1. pd.get_dummies : 명목형 데이터 -----> 숫자 0과 1의 새로운 컬럼으로 생성
		2. LabelEncoder : 명목형 데이터 -------> 명목형 데이터의 종류에 따라 바로 숫자로 변환
										     ( 종류가 3개면 숫자 0,1,2 된다. )


# 정답컬럼을 0 , 1, 2 로 변경합니다.

y = iris.iloc[ : , 4]           # 정답컬럼

from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
encoder.fit(y)
y2 = encoder.transform(y)              # 0, 1, 2 로 변경함























