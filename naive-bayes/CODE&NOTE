▩ 나이브 베이즈 알고리즘이 사용이 되는 분야 ?

	1. 스팸 이메일 필터링과 같은 텍스트 분류
	2. 컴퓨터 네트워크에서 침입이나 비정상적인 행위 탐지
	3. 일련의 관찰된 증상에 따른 의학적 질병 진단

▩ 나이브 베이즈 이론 설명

 R 목차 -------> 나이브 베이즈 이론 PPT

예제 1. 비아그라가 포함되어져 있는 메일이 스팸메일일 확률을 구하시오 !


                         P( 스팸 ∩ 비아그라  )
P(스팸|비아그라) = -----------------------------------
                              P( 비아그라 )




			                          빈도표     ------------------------------->  우도표
           
                        비아그라                                                 비아그라
                    예            아니오     총계                       예          아니오     총계
스팸                 4             16         20                       4/20        16/20      20/100
햄                   1             79         80                       1/80        79/80      80/100
                     5             95         100                      5/100       95/100      100


                            P( 스팸 ∩ 비아그라  )                     4/20  * 20/100
P(스팸|비아그라) = -----------------------------------   =  ----------------------------------- = 0.8
                               P( 비아그라 )                               5/100

0.5보다 크니까 스팸이다.

▩ 비아그라 뿐만 아니라 다른 단어들도 여러개 있는 경우의 스팸일 확률 ?

P ( 스팸|비아그라 ∩ 돈 ∩ 식료품 ∩ 주소삭제 ) = ?

비아그라 단어 하나만 가지고 스팸 메일인지를 분류하려면 정확하게 분류가 안될 수 있으니
다른 단어들도 같이 포함시켜야 합니다.

수학기호 : ￢ : 존재하지 않는다, 포함하지 않는다. ( 부정 ) ( 'ㄷ' 한자 )
		      ∃ : 존재한다, 포함한다. ( 부정 )

P ( 스팸|비아그라 ∩ ￢돈 ∩ ￢식료품 ∩ 구독취소 ) = ?

비아그라와 구독취소는 포함되어져 있는데 돈과 식료품은 포함되지 않은 메일이 스팸일 확률 ?


                   P( B|A ) * P(A)                 P (비아그라 ∩ ￢돈 ∩ ￢식료품 ∩ 구독취소| 스팸 ) * P( 스팸 )
P(A|B) = ---------------------------   =  -----------------------------------------------------------------------------------------------
                            P( B )                                     P ( 비아그라 ∩ ￢돈 ∩ ￢식료품 ∩ 구독취소 )


      P ( 비아그라|스팸 ) * P ( ￢돈|스팸 ) * P ( ￢식료품|스팸 ) * P ( 구독취소|스팸  ) * P ( 스팸 )
=  ---------------------------------------------------------------------------------------------------------------------------------------
                                        P ( 비아그라 ∩ ￢돈 ∩ ￢식료품 ∩ 구독취소 )
       

      P ( 비아그라|스팸 ) * P ( ￢돈|스팸 ) * P ( ￢식료품|스팸 ) * P ( 구독취소|스팸  ) * P ( 스팸 )
=  ---------------------------------------------------------------------------------------------------------------------------------------
                                               분모 무시 ( >> 책 157 페이지  )

분모는 타깃 클래스 ( 스팸이나 햄 ) 에 종속되지 않기 때문에 상수값으로 취급하며, 당분간 무시할 수 있다.
따라서 스팸에 대한 조건부 확률은 다음과 같이 표현된다.



 P ( 비아그라|스팸 ) * P ( ￢돈|스팸 ) * P ( ￢식료품|스팸 ) * P ( 구독취소|스팸  ) * P ( 스팸 )  = ?
        4/ 20               10 / 20            20 / 20                12 / 20        20/100      =    0.012


 P ( 비아그라|햄 ) * P ( ￢돈|햄 ) * P ( ￢식료품|햄 ) * P ( 구독취소|햄  ) * P ( 햄 ) = ?
        1 / 80            66 / 80          71/80             23 / 80        80 / 100     =      0.002

스팸의 전체 우도 ? 0.012
햄의 전체 우도 ? 0.002

                           0.012
스팸일 확률 ?   ------------------------  = 0.857
                       0.012 + 0.002


                        0.002
햄일 확률 ?   ---------------------- = 0.143
                   0.012 + 0.002


이 메세지에 있는 단어 패턴에 대해 85.7 % 확률로 메세지가 스팸이고
14.3% 의 확률로 햄이라고 예상한다.

■ 라플라스 추정기 


 P ( 비아그라|스팸 ) * P ( ￢돈|스팸 ) * P ( ￢식료품|스팸 ) * P ( 구독취소|스팸  ) * P ( 스팸 )  = ?
       4/ 20               10 / 20             0 / 20              12 / 20           20/100      =         0


하나가 0 이 되버리면 ( 식료품 ) 다 0이 되버린다...에바..

위와 같이 분자하나가 0 이면 전체가 0 이 되면서 스팸의 우도가 0 이 되어 버립니다.
그러면서 더 이상 계산을 진행 할 수 없게 됩니다.
그래서 수학자가 라플라스가 어떻게 했냐면 ?
0을 1로 만들어 주면서(모든 항에 1씩 더해줌, 전체는 행의 개수만큼) 아래와 같이 


 P ( 비아그라|스팸 ) * P ( ￢돈|스팸 ) * P ( ￢식료품|스팸 ) * P ( 구독취소|스팸  ) * P ( 스팸 )  = ?
      4+1/ 20 +4         10 +1 / 20 +4       0 +1 / 20 +4        12 + 1/ 20 +4   20 +4 /100 +4      =         


P ( 비아그라|햄 ) * P ( ￢돈|햄 ) * P ( ￢식료품|햄 ) * P ( 구독취소|햄  ) * P ( 햄 ) = ?
     2 / 84             67 / 84          72/84              24 / 84        84 / 104                   

아주 작은값을 하나 더해서 계산이 될 수 있도록하는데 이 값을 라플라스 값이라고 합니다.

라플라스 값을 주어서 나이브베이즈 모델의 성능을 올리는데 사용합니다.

하이퍼 파라미터 ? 머신러닝의 성능을 높이기 위해서 모형 개발자가 직접 조정해줘야하는 파라미터

※ knn 일때는 k 값이 하이퍼 파라미터 였는데 나이브 베이즈에서는 라플라스 값이 하이퍼 파라미터입니다.

▩ R 을 이용해서 나이브 베이즈 머신러닝 모델 만들기

	" 정상버섯과 독버섯을 분류하는 머신러닝 모델을 생성하기 "
	
	데이터 셋 : 데이터 게시판 mushrooms.csv

# 1. 데이터를 로드합니다.
# 2. 결측치를 확인합니다.
# 3. 이상치를 확인합니다.
# 4. 명목형 데이터가 있는지 확인합니다.
# 5. 데이터를 정규화 합니다.
# 6. 훈련 데이터와 테스트 데이터를 분리합니다.
# 7. 나이브 베이즈 모델을 생성합니다.
# 8. 훈련 데이터와 라벨( 정답 )으로 모델을 훈련시킵니다.
# 9. 훈련된 모델로 테스트 데이터를 예측합니다.
# 10. 모델의 성능을 평가합니다.
# 11. 모델의 성능을 높입니다.


# 1. 데이터를 로드합니다.

mush <- read.csv("mushrooms.csv", stringsAsFactor = TRUE)
str(mush)         # all factor

# 맨앞에 있는 type 이 라벨(정답) 입니다.

table(mush$type)

#     e    p                  ( edible   poisonous   )
# 4208 3916 

prop.table(table( mush$type ))                    #  비율을 알수있음.

#         e         p 
# 0.5179714 0.4820286 

# 두개가 딱 절반이어서 독버섯도 잘 학습할 수 있고 정상버섯도 잘 학습할 수 있게 되어있습니다.

dim(mush)                 # [1] 8124   23 , 전체 건수가 어떻게 되는지 확인

# 2. 결측치를 확인합니다.

colSums(is.na(mush))                    # 모두 0 이다.

# 3. 이상치를 확인합니다.

명목형 데이터여서 이상치를 확인할 필요가 없습니다.

# 4. 명목형 데이터가 있는지 확인합니다.

전부 명목형 데이터 입니다.

# 5. 데이터를 정규화 합니다.

전부 명목형 데이터 이므로 정규화 작업도 필요하지 않습니다.

# 6. 훈련 데이터와 테스트 데이터를 분리합니다.

# 데이터 shuffle 과 데이터 분리를 효율적이면서도 편하게 수행할 수 있는 패키지를 이용해서 분리해보겠습니다.

install.packages("caret")
library(caret)

set.seed(1)
k <- createDataPartition( mush$type, p = 0.8 , list = F )   # 훈련데이터 80 %, 테스트 20 %  / sample 사용하지 않고 쉽게 만듦
												    # 또한, 리스트 형태로 만들지 말아라
k           # 훈련데이터의 index가 나옴

train_data <- mush[ k, ]
test_data <- mush[ -k,  ]

dim(train_data)           # [1] 6500   23
dim(test_data)           # [1] 1624   23

prop.table( table(train_data$type) ) 

#  e     p 
# 0.518 0.482

prop.table( table(test_data$type) )

#         e         p 
# 0.5178571 0.4821429 

※ 훈련데이터와 테스트 데이터의 독버섯과 정상버섯이 거의 50:50으로 균등하게 분포되어있습니다.
    sample에 비해서 데이터가 더 훈련하고 테스트하기 쉽게 잘 나눠져서 좋다.


# 7. 나이브 베이즈 모델을 생성합니다.

install.packages("e1071")
library(e1071)

model <- naiveBayes( type~ . , data= train_data )        # type 는 라벨 ,  '.'은 라벨외의 모든컬럼을 뜻함
model


# cap_shape = 버섯 cap의 모양
# cap_surface = 버섯 cap의 표면,, 등등

# 버섯 데이터로 빈도표를 만들고서 우도표를 생성했다.

# 8. 훈련 데이터와 라벨( 정답 )으로 모델을 훈련시킵니다.

# 7번에서 다 수행했습니다.


# 9. 훈련된 모델로 테스트 데이터를 예측합니다.

result <- predict( model, test_data[  , -1] )              # 정답 컬럼 빼고

# 테스트 데이터의 정답을 제외하고 예측합니다.
result

# 10. 모델의 성능을 평가합니다.

sum ( (result == test_data[, 1]) / length(test_data[ , 1]) )         # [1]  0.9378079

어제는 유방암 데이터가 전부 숫자여서 knn 알고리즘을 이용해서 기계학습 시켰고
오늘은 독버섯 데이터가 전부 명목형이어서 naivebayes 를 이용해서 기계학습 시켰습니다.


# 11. 모델의 성능을 높입니다.

model2 <- naiveBayes( type~ . , data = train_data, laplace = 0.0001 )

result2 <- predict( model2, test_data[  , -1] )

sum ( (result2 == test_data[, 1]) / length(test_data[ , 1]) )          # 0.9950739

### 직업과 성별과 결혼 유무등의 데이터를 가지고 영화장르를 예측하는데
		   나이브 베이즈 모델을 생성하시오 ! ( 데이터 셋 : movie.csv )

# 1. 데이터를 로드합니다.

movie <- read.csv("movie.csv", stringsAsFactor = TRUE)
str(movie)         # all factor

# 맨앞에 있는 type 이 라벨(정답) 입니다.

table(movie$장르)

   #  SF   공포 로맨틱   무협 스릴러   액션 코미디 
   #  5      5      9      5      5      5      5 


prop.table(table( movie$장르 ))                    #  비율을 알수있음.

 #      SF      공포    로맨틱      무협    스릴러      액션    코미디 
# 0.1282051 0.1282051 0.2307692 0.1282051 0.1282051 0.1282051 0.1282051

dim(mush)                 # [1] [1] 39  6 , 전체 건수가 어떻게 되는지 확인

# 2. 결측치를 확인합니다.

colSums(is.na(movie))                    # 모두 0 이다.

# 3. 이상치를 확인합니다.

명목형 데이터여서 이상치를 확인할 필요가 없습니다.

# 4. 명목형 데이터가 있는지 확인합니다.

전부 명목형 데이터 입니다.

# 5. 데이터를 정규화 합니다.

전부 명목형 데이터 이므로 정규화 작업도 필요하지 않습니다.

# 6. 훈련 데이터와 테스트 데이터를 분리합니다.

# 데이터 shuffle 과 데이터 분리를 효율적이면서도 편하게 수행할 수 있는 패키지를 이용해서 분리해보겠습니다.

install.packages("caret")
library(caret)

set.seed(1)
k <- createDataPartition( movie$장르, p = 0.8 , list = F )   # 훈련데이터 80 %, 테스트 20 %  / sample 사용하지 않고 쉽게 만듦
												    # 또한, 리스트 형태로 만들지 말아라
k           # 훈련데이터의 index가 나옴

train_data <- movie[ k, ]
test_data <- movie[ -k,  ]

dim(train_data)           # [1] 32  6
dim(test_data)           # [1] 7 6

prop.table( table(train_data$장르) ) 

#    SF   공포 로맨틱   무협 스릴러   액션 코미디 
#  0.125  0.125  0.250  0.125  0.125  0.125  0.125 

prop.table( table(test_data$장르) )

#        SF      공포    로맨틱      무협    스릴러      액션    코미디 
# 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571 0.1428571

※ 훈련데이터와 테스트 데이터의 독버섯과 정상버섯이 거의 50:50으로 균등하게 분포되어있습니다.
    sample에 비해서 데이터가 더 훈련하고 테스트하기 쉽게 잘 나눠져서 좋다.


# 7. 나이브 베이즈 모델을 생성합니다.

install.packages("e1071")
library(e1071)

model <- naiveBayes( type~ . , data= train_data )        # type 는 라벨 ,  '.'은 라벨외의 모든컬럼을 뜻함
model


# cap_shape = 버섯 cap의 모양
# cap_surface = 버섯 cap의 표면,, 등등

# 버섯 데이터로 빈도표를 만들고서 우도표를 생성했다.

# 8. 훈련 데이터와 라벨( 정답 )으로 모델을 훈련시킵니다.

# 7번에서 다 수행했습니다.


# 9. 훈련된 모델로 테스트 데이터를 예측합니다.

result <- predict( model, test_data[  , -1] )              # 정답 컬럼 빼고

# 테스트 데이터의 정답을 제외하고 예측합니다.
result

# 10. 모델의 성능을 평가합니다.

sum ( (result == test_data[, 1]) / length(test_data[ , 1]) )         # 

어제는 유방암 데이터가 전부 숫자여서 knn 알고리즘을 이용해서 기계학습 시켰고
오늘은 독버섯 데이터가 전부 명목형이어서 naivebayes 를 이용해서 기계학습 시켰습니다.


# 11. 모델의 성능을 높입니다.

model2 <- naiveBayes( type~ . , data = train_data, laplace = 0.0001 )

result2 <- predict( model2, test_data[  , -1] )

sum ( (result2 == test_data[, 1]) / length(test_data[ , 1]) )          # 


#####   요약  #####

movie <- read.csv("movie.csv", stringsAsFactor = TRUE)

set.seed(1)
k <- createDataPartition( movie$장르, p = 0.8 , list = F )   # 훈련데이터 80 %, 테스트 20 %  / sample 사용하지 않고 쉽게 만듦
												    # 또한, 리스트 형태로 만들지 말아라
train_data <- movie[ k, ]
test_data <- movie[ -k,  ]
model <- naiveBayes( 장르~ . , data= train_data )        # type 는 라벨 ,  '.'은 라벨외의 모든컬럼을 뜻함
result <- predict( model, test_data[  , -6] )              # 정답 컬럼 빼고
sum ( (result == test_data[, 6]) / length(test_data[ , 6]) )         # [1] 1

model2 <- naiveBayes( 장르~ . , data = train_data, laplace = 0.0001 )
result2 <- predict( model2, test_data[  , -6] )
sum ( (result2 == test_data[, 6]) / length(test_data[ , 6]) )          #  [1] 1

### 직업이 학생이고 결혼은 안했으며 이성친구가 없는 20대 남자가 선호하는 영화 장르가 
		   무엇으로 예측되는지 지금 방금 만든 나이브 베이즈 모델에 데이터를 입력해서 출력하시오 !

	1. 방금생성한 정확도 100% 의 모델로 예측

test_data3 <- data.frame( 나이 = '20대', 성별 = '남', 직업 ='학생', 결혼여부 = 'NO', 이성친구 ='NO' )

result3 <- predict( model2, test_data3, laplace = 0 )
result3                               # 코미디


### 독감 데이터로 나이브 베이즈 모델로 생성해서 독감환자인지 아닌지 분류하는 모델을 만드시오
		  ( flu.csv )

patient_id  : 환자번호
chills : 오한
runny_nose : 콧물
headache : 두통
fever : 열
flue : 독감여부



flu <- read.csv("flu.csv", stringsAsFactor = TRUE)

set.seed(1)
k <- createDataPartition( flu$flue, p = 0.8 , list = F )   # 훈련데이터 80 %, 테스트 20 %  / sample 사용하지 않고 쉽게 만듦
												    # 또한, 리스트 형태로 만들지 말아라
train_data <- flu[ k, ]
test_data <- flu[ -k,  ]
model <- naiveBayes( flue~ . , data= train_data )        # type 는 라벨 ,  '.'은 라벨외의 모든컬럼을 뜻함
result <- predict( model, test_data[  , -6] )              # 정답 컬럼 빼고
sum ( (result == test_data[, 6]) / length(test_data[ , 6]) )         # [1]  1

model2 <- naiveBayes( flue~ . , data = train_data, laplace = 0.0001 )
result2 <- predict( model2, test_data[  , -6] )
sum ( (result2 == test_data[, 6]) / length(test_data[ , 6]) )          #  [1] 1


###########################################################################################################################################


▩ 파이썬으로 나이브 베이즈 모델 생성하기

# 1. 데이터를 로드합니다.

import pandas as pd
mush = pd.read_csv("c:\\data\\mushrooms.csv")

# 2. 결측치를 확인합니다.

print ( mush.isnull().sum() )

# 3. 이상치를 확인합니다.

# 모두 명목형이므로 이상치 확인 불가

# 4. 명목형 데이터가 있는지 확인합니다.

# R 에서는 데이터를 명목형인 상태 그대로 훈련을 시켰는데 파이썬에서는 전부 숫자로 변경해줘야 합니다.

# 정답을 뺀 데이터만 선별합니다.

x = mush.iloc[ : , 1: ]
print( x.head() )

# 정답 데이터를 y 변수에 담습니다.

y = mush.iloc [  : , 0]
print ( y.head() )

# 명목형 데이터를 숫자로 변경합니다.

mush2 = pd.get_dummies(x)
print( mush2.head() )
print( mush2.shape )            # (8124, 117) , 컬럼의 갯수가 23개에서 117개로 늘어났습니다.
print( mush2.info() )             # 전부 숫자인지 확인합니다.

# 5. 데이터를 정규화 합니다.

print ( mush2.describe() )

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(mush2)
mush2_scaled = scaler.transform(mush2)
print( mush2_scaled )       # numpy array 형태로 생성함

y = y.to_numpy()      # 위에서 만든 정답 데이터를 numpy 형태로 만들어줌
print( y )


# 6. 훈련 데이터와 테스트 데이터를 분리합니다.

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split ( mush2_scaled, y , test_size = 0.2, random_state = 1 )

print(x_train.shape)         #  (6499, 117)
print(x_test.shape)               #  (1625, 117)


# 7. 나이브 베이즈 모델을 생성합니다.

	1. BernoulliNB : 이산형 데이터를 분류할 때 적합
	2. GaussianNB : 연속형 데이터를 분류할 때 적합
	3. MultinomialNB : 이산형 데이터를 분류할 때 적합

from sklearn.naive_bayes import BernoulliNB
# from sklearn.naive_bayes import GaussianNB
# from sklearn.naive_bayes import MultinomialNB

model = BernoulliNB()

# 8. 훈련 데이터와 라벨( 정답 )으로 모델을 훈련시킵니다.

model.fit(x_train, y_train)

# 9. 훈련된 모델로 테스트 데이터를 예측합니다.

result = model.predict(x_test)

# 10. 모델의 성능을 평가합니다.

print ( (sum( result == y_test )) / (len(y_test)) * 100)          # 93.9076923076923

from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test, result)
print( accuracy )              # 0.939076923076923

from sklearn.metrics import confusion_matrix

a = confusion_matrix( y_test, result )
print(a)
                                    #      식용 독버섯
# [[815   5]        식용          TN        FP                     
#  [ 94 711]]      독버섯      FN        TP
# FN 값이 너무 높다.. 독버섯인데 식용으로 판단하면 죽는사람 많아짐,, ( 베르누이NB 가 성능이 안좋네 )

tn, fp, fn, tp = confusion_matrix( y_test, result ).ravel()
print(tn, fp, fn, tp)


# 11. 모델의 성능을 높입니다.

# from sklearn.naive_bayes import BernoulliNB
from sklearn.naive_bayes import GaussianNB
# from sklearn.naive_bayes import MultinomialNB

model2 = GaussianNB( var_smoothing = 0.001 )
model2.fit(x_train, y_train)
result2 = model2.predict(x_test)

print ( (sum( result2 == y_test )) / (len(y_test)) * 100)          # 99.50769230769231

from sklearn.metrics import confusion_matrix
a = confusion_matrix( y_test, result2 )
print(a)

# [[814   6]
#   [  2 803]]




##### 요약 #####

import pandas as pd
mush = pd.read_csv("c:\\data\\mushrooms.csv")

x = mush.iloc[ : , 1: ]
y = mush.iloc [  : , 0]

mush2 = pd.get_dummies(x)

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
scaler.fit(mush2)
mush2_scaled = scaler.transform(mush2)

y = y.to_numpy()      # 위에서 만든 정답 데이터를 numpy 형태로 만들어줌

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split ( mush2_scaled, y , test_size = 0.2, random_state = 1 )

from sklearn.naive_bayes import BernoulliNB

model = BernoulliNB()

model.fit(x_train, y_train)

result = model.predict(x_test)

print ( (sum( result == y_test )) / (len(y_test)) * 100)          # 93.9076923076923

from sklearn.metrics import confusion_matrix

a = confusion_matrix( y_test, result )
print(a)

# 성능 향상

# from sklearn.naive_bayes import BernoulliNB
from sklearn.naive_bayes import GaussianNB
# from sklearn.naive_bayes import MultinomialNB

model2 = GaussianNB( var_smoothing = 0.001 )
model2.fit(x_train, y_train)
result2 = model2.predict(x_test)

print ( (sum( result2 == y_test )) / (len(y_test)) * 100)          # 99.50769230769231

from sklearn.metrics import confusion_matrix
a = confusion_matrix( y_test, result2 )
print(a)

### for loop 문을 이용해서 GaussianNB 모델의 FN 값이 0 이 되는 var_smoothing 값이
		  무엇인지 알아 내시오 !

for i in range(1, 101):
    # from sklearn.naive_bayes import BernoulliNB
    from sklearn.naive_bayes import GaussianNB
    # from sklearn.naive_bayes import MultinomialNB

    model2 = GaussianNB( var_smoothing = (i /1000 ) ) 
    model2.fit(x_train, y_train)
    result2 = model2.predict(x_test)

    from sklearn.metrics import confusion_matrix
    a = confusion_matrix( y_test, result2 )
    tn, fp, fn, tp = confusion_matrix( y_test, result2 ).ravel()

    if fn == 0:
        print ( (sum( result2 == y_test )) / (len(y_test)) * 100)          # 안나옴,, fn이 2인값이 최선인듯
        print(a)


※ 결론 : 독버섯 데이터를 나이브베이즈 모델중 GaussianNB 로 학습 시켰을때 정확도는
		 99.50 까지 개선되었습니다.
		그런데 FN 값이 2개가 나타나고 있어서 독버섯 데이터를 분류하는 모델은 FN 값을 0으로
		만드는게 중요하므로 모델을 다른 모델로 변경해서 0으로 만들어야 합니다.
		
*현업에서 머신러닝 데이터 분석시 가장 중요한 것? 마감날짜 엄수
  마감 날짜때까지 나온 데이터 분석 결과를 알려줘야함!!

▩ 명목형 변수를 숫자로 변경해주는 코드 2가지

	1. pd.get_dummies : 컬럼을 만들어서 0과 1로 데이터를 생성
						     컬럼                데이터                                  컬럼
					  예 ) 버섯색깔 : red, blue, green -----------> red , blue, green
															 0       0        0           데이터
															 1       1        1
	
	2. LabelEncoder : 컬러안에 있는 명목형을 숫자로 변경
					      컬럼                 데이터                    데이터 
				  예 ) 버섯색깔 : red, blue, green --------> 0 , 1, 2


#### 나이가 20대이고 성별이 여자이며 직업이 IT 이고 결혼을 안했으며
		   이성친구가 없는 사람이 선택할 가능성이 높은 영화 장르는 ?  ( 파이썬으로, moive.csv )


import pandas as pd
movie = pd.read_csv("c:\\data\\movie.csv", encoding = 'euckr')
# print ( movie.isnull().sum() )     # 결측치확인

x = movie.iloc[ : , :5 ]            # 정답을 뺀 데이터 생성
y = movie.iloc [  : , 5]            # 정답 데이터 생성

movie2 = pd.get_dummies(x)  #  5개 컬럼에서 18개로 늘어남 , 숫자 데이터로 변환

from sklearn.preprocessing import MinMaxScaler         # 데이터를 정규화 합니다.

scaler = MinMaxScaler()
scaler.fit(movie2)
movie2_scaled = scaler.transform(movie2)

y = y.to_numpy()           # y를 numpy로 만들어줌

from sklearn.model_selection import train_test_split       # 훈련데이터와 테스트 데이터 분리

x_train, x_test, y_train, y_test = train_test_split ( movie2_scaled, y , test_size = 0.2, random_state = 1 )

from sklearn.naive_bayes import BernoulliNB      # 나이브 베이즈 모델 생성
 
model = BernoulliNB()

model.fit(x_train, y_train)          # 모델 훈련

result = model.predict(x_test)          # 예측

print ( (sum( result == y_test )) / (len(y_test)) * 100)          # 87.5

# 성능 향상 & var_smoothing 값 찾기

x_val = list(range(1,1000))
y_val = []

for i in range(1, 1000):
    # from sklearn.naive_bayes import BernoulliNB
    from sklearn.naive_bayes import GaussianNB
    # from sklearn.naive_bayes import MultinomialNB

    model2 = GaussianNB( var_smoothing = (i /1000 ) ) 
    model2.fit(x_train, y_train)
    result2 = model2.predict(x_test)

    from sklearn.metrics import confusion_matrix
    a = sum( result2 == y_test ) / (len(y_test)) * 100
    y_val.append(a)    
                              
plot_dict = { 'i' : x_val, '확률' : y_val }
plot_dict2 = pd.DataFrame(plot_dict)
print(plot_dict2.plot(kind = 'line', x = 'i', y = '확률'))


# i = 599 이상일때 ( var_smoothing가 0.599 이상 ) 확률이 100이다 (a == 100) 

######  예측할 데이터 생성

print(movie2.columns)       # 숫자로 변형한 데이터의 컬럼 확인
						
# ['나이_10대', '나이_20대', '나이_30대', '나이_40대', '성별_남', '성별_여', '직업_IT',
# '직업_디자이너', '직업_무직', '직업_언론', '직업_영업', '직업_자영업', '직업_학생', '직업_홍보/마케팅',
 # '결혼여부_NO', '결혼여부_YES', '이성친구_NO', '이성친구_YES']

exp_data = {}
temp_list = [0]*len(movie2.columns)

for i,k in zip(movie2.columns, temp_list ):
    exp_data[i] = k 

exp_data['나이_20대'] = 1
exp_data['직업_IT'] = 1
exp_data['성별_여'] = 1
exp_data['결혼여부_NO'] = 1
exp_data['이성친구_NO'] = 1

exp_data2 = pd.DataFrame ( exp_data, index = [0] )   

# 결과 예측
result2 = model2.predict(exp_data2.to_numpy())          # 예측
print(result2)                   # 정답 : 로맨틱

